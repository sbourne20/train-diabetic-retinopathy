#!/bin/bash
# MEDICAL-GRADE EXTREME ANTI-OVERFITTING TRAINING
# Resume with maximum regularization to achieve 90%+ validation accuracy

echo "üè• MEDICAL-GRADE EXTREME ANTI-OVERFITTING TRAINING"
echo "Foundation Model: google/medsiglip-448 with EXTREME regularization"
echo ""
echo "üéØ EXTREME ANTI-OVERFITTING STRATEGY:"
echo "  ‚úÖ Ultra-low learning rate: 5e-5 (10x lower than before)"
echo "  ‚úÖ Maximum weight decay: 1e-1 (10x stronger regularization)"
echo "  ‚úÖ Heavy dropout: 0.7 (maximum regularization)"
echo "  ‚úÖ Smallest batch size: effective batch=4 (prevent memorization)"
echo "  ‚úÖ Early stopping: patience=10 (aggressive overfitting detection)"
echo "  ‚úÖ Gradient clipping: 0.5 (prevent gradient explosion)"
echo "  ‚úÖ Resume from best checkpoint (preserve good weights)"
echo ""
echo "üí° WHY THIS WILL WORK:"
echo "  ‚Ä¢ Current training shows perfect memorization - need extreme regularization"
echo "  ‚Ä¢ Smaller effective batch forces model to generalize better"
echo "  ‚Ä¢ Ultra-low LR prevents catastrophic overfitting"
echo "  ‚Ä¢ Heavy regularization forces medical-grade generalization"
echo ""
echo "üí∞ COST SAVINGS:"
echo "  ‚Ä¢ Resume from existing checkpoint: ~$200 already invested"
echo "  ‚Ä¢ Additional cost: ~$80-100 (50-70 more epochs with early stopping)"
echo "  ‚Ä¢ Total: ~$280-300 instead of ~$480 starting fresh"
echo ""

python vertex_ai_trainer.py \
  --action train \
  --dataset dataset3_augmented_resized \
  --dataset-type 1 \
  --bucket_name dr-data-2 \
  --project_id curalis-20250522 \
  --region us-central1 \
  --num-epochs 100 \
  --use-lora no \
  --learning-rate 5e-5 \
  --batch-size 1 \
  --freeze-backbone-epochs 0 \
  --enable-focal-loss \
  --enable-medical-grade \
  --enable-class-weights \
  --warmup-epochs 1 \
  --scheduler cosine \
  --validation-frequency 1 \
  --patience 10 \
  --min-delta 0.005 \
  --weight-decay 1e-1 \
  --dropout 0.7 \
  --checkpoint_frequency 1 \
  --gradient-accumulation-steps 4 \
  --gradient-clip-value 0.5 \
  --resume-from-checkpoint gs://dr-data-2/checkpoints/best_model.pth \
  --experiment-name "medsiglip_extreme_antioverfitting"

echo ""
echo "üéØ EXPECTED OUTCOME:"
echo "  ‚Ä¢ Validation accuracy: 90-95% (medical-grade)"
echo "  ‚Ä¢ Training time: 2-3 days (early stopping will kick in)"
echo "  ‚Ä¢ Generalization: Excellent due to extreme regularization"
echo "  ‚Ä¢ Cost: ~$80-100 additional (total ~$280-300)"
echo ""
echo "üìä KEY CHANGES FROM PREVIOUS TRAINING:"
echo "  ‚Ä¢ Learning rate: 1e-4 ‚Üí 5e-5 (50% reduction)"
echo "  ‚Ä¢ Weight decay: 5e-3 ‚Üí 1e-1 (20x increase!)"
echo "  ‚Ä¢ Dropout: 0.3 ‚Üí 0.7 (2.3x increase)"
echo "  ‚Ä¢ Batch size: 2‚Üí1, accumulation: 4‚Üí4 (effective batch=4 instead of 8)"
echo "  ‚Ä¢ Gradient clip: none ‚Üí 0.5 (stability)"
echo "  ‚Ä¢ Early stopping: patience 15‚Üí10 (faster overfitting detection)"