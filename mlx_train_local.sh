#!/bin/bash
# MLX MEDICAL-GRADE LOCAL TRAINING - MAC M4 NUCLEAR FOCAL LOSS FOR IMBALANCED DATASET3
# Optimized for severe class imbalance with aggressive focal loss parameters

echo "ðŸŽ MLX LOCAL TRAINING: NUCLEAR FOCAL LOSS FOR IMBALANCED DATASET3 - MAC M4"
echo "Foundation Model: google/medsiglip-448 - AGGRESSIVE PARAMETERS FOR SEVERE IMBALANCE"
echo ""
echo "ðŸš€ NUCLEAR FOCAL LOSS: Aggressive parameters for severe class imbalance"
echo "  ðŸŽ¯ TARGET: Handle 5.4x imbalance â†’ 85%+ â†’ 92% medical-grade accuracy"
echo "  âš ï¸  Imbalanced Dataset3: 118k samples with 5.4:1 imbalance ratio"
echo "  âœ… Hardware: Apple Silicon M4 (MLX optimized for efficiency)"
echo "  âœ… Memory Optimized: LoRA r=16 for Apple Silicon efficiency"
echo ""
echo "ðŸŽ¯ NUCLEAR FOCAL LOSS CONFIGURATION:"
echo "  âœ… LoRA Rank (r): 16 (memory efficient configuration)"
echo "  âœ… LoRA Alpha: 32 (proven effective configuration)"
echo "  ðŸŽ¯ Learning Rate: 2e-5 (stable rate for imbalanced training)"
echo "  ðŸ”¥ Class Weights: 8.0/8.0 (NUCLEAR: extreme priority for Classes 3&4)"
echo "  ðŸš€ Scheduler: none (STABLE: fixed LR throughout training)"
echo "  âœ… Medical Warmup: 20 epochs (gradual ramp-up for stability)"
echo "  ðŸŽ¯ Batch Size: 2 (MLX OPTIMIZED: with gradient accumulation)"
echo "  âœ… Dropout: 0.3 (regularization for large dataset)"
echo "  âœ… Weight Decay: 1e-5 (light regularization)"
echo "  ðŸ”¥ Focal Loss: Î±=4.0, Î³=6.0 (NUCLEAR: aggressive focus on hard examples)"
echo ""
echo "ðŸŽ MLX APPLE SILICON ADVANTAGES:"
echo "  â€¢ Memory Efficiency: Unified memory architecture"
echo "  â€¢ Memory Safety: Auto-limits to 70% of system memory"
echo "  â€¢ Memory Monitoring: Real-time usage tracking and warnings"
echo "  â€¢ Memory Cleanup: Periodic cache clearing every 50 batches"
echo "  â€¢ Speed: Native Apple Silicon optimization via MLX"
echo "  â€¢ Stability: Local training with full system control"
echo "  â€¢ Cost: Zero cloud compute charges"
echo "  â€¢ Debug Friendly: Real-time monitoring and adjustment"
echo "  â€¢ Batch Tracking: Progress bars with memory usage display"
echo ""

# Check if dataset3_augmented_resized exists
if [ ! -d "dataset3_augmented_resized" ]; then
    echo "âŒ Error: dataset3_augmented_resized folder not found!"
    echo "Please ensure dataset3_augmented_resized exists with train/val/test structure."
    exit 1
fi

# Check if MLX is installed
python3 -c "import mlx.core" 2>/dev/null
if [ $? -ne 0 ]; then
    echo "âŒ Error: MLX not installed!"
    echo "Install with: pip install mlx"
    exit 1
fi

# Check if psutil is installed for memory management
python3 -c "import psutil" 2>/dev/null
if [ $? -ne 0 ]; then
    echo "ðŸ“¦ Installing psutil for memory management..."
    pip install psutil || echo "âš ï¸ psutil installation failed"
fi

# Check for HuggingFace token
if [ -z "$HUGGINGFACE_TOKEN" ] && [ ! -f ".env" ]; then
    echo "âŒ Error: HUGGINGFACE_TOKEN not found!"
    echo "Create .env file with: HUGGINGFACE_TOKEN=hf_your_token_here"
    exit 1
fi

echo "ðŸ’¾ MODEL CACHING INFO:"
echo "  â€¢ First run: Downloads MedSigLIP-448 (3.5GB) to results/models/"
echo "  â€¢ Subsequent runs: Loads from cache instantly"
echo "  â€¢ To pre-download: python3 download_model.py"
echo ""
echo "ðŸš€ STARTING LOCAL MLX TRAINING..."
echo ""

# Run MLX training with NUCLEAR FOCAL LOSS parameters for imbalanced dataset3
python3 mlx_ai_trainer.py \
  --dataset-path dataset3_augmented_resized \
  --results-dir results \
  --num-epochs 50 \
  --batch-size 2 \
  --learning-rate 2e-5 \
  --weight-decay 1e-5 \
  --dropout 0.3 \
  --scheduler none \
  --gradient-accumulation-steps 6 \
  --warmup-epochs 20 \
  --validation-frequency 1 \
  --checkpoint-frequency 2 \
  --patience 15 \
  --min-delta 0.001 \
  --enable-focal-loss \
  --focal-loss-alpha 4.0 \
  --focal-loss-gamma 6.0 \
  --enable-class-weights \
  --class-weight-severe 8.0 \
  --class-weight-pdr 8.0 \
  --enable-medical-grade \
  --gradient-clip-norm 1.0 \
  --use-lora \
  --lora-r 16 \
  --lora-alpha 32 \
  --experiment-name "mlx_medsiglip_lora_nuclear_focal_imbalanced_dataset3"

echo ""
echo "â±ï¸ MLX NUCLEAR FOCAL LOSS TRAINING COMPLETED!"
echo "ðŸ“Š Results saved to: results/"
echo "ðŸ’¾ Checkpoints saved to: results/checkpoints/"
echo "ðŸ“ˆ Outputs and plots saved to: results/outputs/"
echo ""
echo "ðŸŽ¯ MLX NUCLEAR FOCAL LOSS TRAINING TIMELINE:"
echo "  â€¢ Duration: 12-16 hours on M4 (large imbalanced dataset training)"
echo "  â€¢ Validation checks: Every epoch (continuous progress monitoring)"
echo "  â€¢ Imbalance handling: Expected breakthrough epoch 15-25 (nuclear focal loss)"
echo "  â€¢ 85%+ convergence: Expected by epoch 30-40 (aggressive parameters)"
echo ""
echo "ðŸ¥ MEDICAL-GRADE SUCCESS CRITERIA (IMBALANCED DATA):"
echo "  â€¢ Overall validation accuracy: â‰¥85% (nuclear focal loss target)"
echo "  â€¢ Severe NPDR sensitivity: â‰¥90% (critical for patient safety)"
echo "  â€¢ PDR sensitivity: â‰¥95% (sight-threatening detection)"
echo "  â€¢ Minority class performance: Classes 3&4 >85% sensitivity despite 5.4x imbalance"
echo ""