#!/bin/bash
# FULL TRAINING FALLBACK - LOCAL V100 MAXIMUM PERFORMANCE MODE
# Use only if LoRA training doesn't achieve 85%+ accuracy

echo "üî• FULL TRAINING MODE: MAXIMUM PERFORMANCE ON V100"
echo "‚ö†Ô∏è  WARNING: This mode uses 12-14GB memory vs 6GB for LoRA"
echo ""
echo "üéØ FULL TRAINING STRATEGY:"
echo "  ‚Ä¢ Mode: Full model fine-tuning (all parameters trainable)"
echo "  ‚Ä¢ Memory: 12-14GB usage (requires 16GB V100)"
echo "  ‚Ä¢ Speed: Slower than LoRA but potentially higher accuracy ceiling"
echo "  ‚Ä¢ Batch Size: Reduced to 2-3 due to memory constraints"
echo "  ‚Ä¢ Target: 88-92% validation accuracy"
echo ""
echo "üö® USE FULL TRAINING ONLY IF:"
echo "  ‚Ä¢ LoRA training plateaus below 85%"
echo "  ‚Ä¢ You need maximum possible accuracy"
echo "  ‚Ä¢ You have sufficient time for longer training"
echo ""
echo "üíæ V100 MEMORY OPTIMIZATION FOR FULL TRAINING:"
echo "  ‚Ä¢ Batch Size: 3 (reduced from 6 to fit in 16GB)"
echo "  ‚Ä¢ Gradient Accumulation: 6 (maintain effective batch size 18)"
echo "  ‚Ä¢ Mixed Precision: Enabled (reduces memory by ~30%)"
echo "  ‚Ä¢ Gradient Checkpointing: Enabled if needed"
echo ""

# Check if dataset5 exists
if [ ! -d "./dataset5" ]; then
    echo "‚ùå ERROR: dataset5 directory not found in current path"
    echo "Please ensure dataset5 exists with train/val/test structure"
    exit 1
fi

# Confirm full training mode
echo "‚ö†Ô∏è  WARNING: Full training will use 12-14GB of your 16GB V100"
echo "Press Enter to continue with full training, or Ctrl+C to cancel..."
read -r

echo "‚úÖ dataset5 found - proceeding with FULL TRAINING mode"
echo ""

# Run local training with FULL MODEL parameters (no LoRA)
python local_trainer.py \
  --mode train \
  --dataset_path ./dataset5 \
  --num_classes 5 \
  --pretrained_path google/medsiglip-448 \
  --img_size 448 \
  --epochs 80 \
  --use_lora no \
  --learning_rate 1e-5 \
  --batch_size 3 \
  --freeze_backbone_epochs 5 \
  --enable_focal_loss \
  --focal_loss_alpha 2.0 \
  --focal_loss_gamma 3.0 \
  --enable_medical_grade \
  --enable_class_weights \
  --class_weight_severe 4.0 \
  --class_weight_pdr 3.0 \
  --gradient_accumulation_steps 6 \
  --warmup_epochs 10 \
  --scheduler polynomial \
  --validation_frequency 2 \
  --patience 20 \
  --min_delta 0.001 \
  --weight_decay 1e-4 \
  --dropout 0.2 \
  --max_grad_norm 0.5 \
  --checkpoint_frequency 5 \
  --experiment_name "medsiglip_FULL_LOCAL_V100_MAXIMUM_PERFORMANCE" \
  --device cuda \
  --medical_terms data/medical_terms_type1.json

echo ""
echo "‚è±Ô∏è FULL TRAINING TIMELINE:"
echo "  ‚Ä¢ Duration: 6-10 hours (longer due to full model training)"
echo "  ‚Ä¢ Memory Usage: 12-14GB V100 (full model parameters)"
echo "  ‚Ä¢ Validation checks: Every 2 epochs (to save time)"
echo "  ‚Ä¢ Expected performance: 88-92% validation accuracy"
echo "  ‚Ä¢ Breakthrough point: Epoch 15-30"
echo ""
echo "üéØ FULL TRAINING SUCCESS CRITERIA:"
echo "  ‚Ä¢ Overall validation accuracy: ‚â•88% (higher ceiling than LoRA)"
echo "  ‚Ä¢ All class sensitivity: ‚â•85%"
echo "  ‚Ä¢ Medical-grade performance: >90% clinical utility"
echo "  ‚Ä¢ Memory stable: No OOM errors throughout training"
echo ""
echo "üî• FULL TRAINING ADVANTAGES:"
echo "  ‚Ä¢ Maximum accuracy potential"
echo "  ‚Ä¢ All parameters optimized for your specific dataset"
echo "  ‚Ä¢ Better feature extraction learning"
echo "  ‚Ä¢ Highest possible medical-grade performance"
echo ""
echo "‚ö†Ô∏è FULL TRAINING CONSIDERATIONS:"
echo "  ‚Ä¢ Higher memory usage (monitor with nvidia-smi)"
echo "  ‚Ä¢ Longer training time"
echo "  ‚Ä¢ Risk of overfitting on smaller dataset"
echo "  ‚Ä¢ Higher computational cost"
echo ""
echo "üöÄ LAUNCHING FULL TRAINING ON V100..."
echo "üíæ USING 12-14GB MEMORY FOR MAXIMUM PERFORMANCE"